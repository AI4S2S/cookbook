{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sea ice concentration with sea surface flux using transformer with multi-head attention\n",
    "This notebook serves as an example of a basic workflow with `s2spy` & `lilio` packages. <br>\n",
    "We will predict temperature in US at seasonal time scales using ERA5 dataset with multi-head attention transformer. <br>\n",
    "\n",
    "This recipe includes the following steps:\n",
    "- Define a calendar (`lilio`)\n",
    "- Download/load input data\n",
    "- Map the calendar to the data (`lilio`)\n",
    "- Train-validate-test split (70%/15%/15%) (`torch`)\n",
    "- Preprocessing based on the training set (`s2spy`)\n",
    "- Resample data to the calendar (`lilio`)\n",
    "- Create transformer model (`torch`)\n",
    "- Specify hyper-parameters\n",
    "- Train model (`torch`)\n",
    "- Evaludate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lilio\n",
    "import numpy as np\n",
    "import time as tt\n",
    "import wandb\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from s2spy import preprocess\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a calendar with `lilio` to specify time range for targets and precursors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NWP calendar for constrained forecasts/hindcasts\n",
    "# create custom calendar based on the time of interest\n",
    "calendar = lilio.Calendar(anchor=\"08-01\")\n",
    "# add target periods\n",
    "calendar.add_intervals(\"target\", length=\"30d\")\n",
    "# add precursor periods\n",
    "periods_of_interest = 6\n",
    "for _ in range(periods_of_interest):\n",
    "    calendar.add_intervals(\"precursor\", \"15d\", gap=\"15d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Calendar(\n",
       "    anchor='08-01',\n",
       "    allow_overlap=False,\n",
       "    mapping=None,\n",
       "    intervals=[\n",
       "        Interval(role='target', length='30d', gap='0d'),\n",
       "        Interval(role='precursor', length='15d', gap='15d'),\n",
       "        Interval(role='precursor', length='15d', gap='15d'),\n",
       "        Interval(role='precursor', length='15d', gap='15d'),\n",
       "        Interval(role='precursor', length='15d', gap='15d'),\n",
       "        Interval(role='precursor', length='15d', gap='15d'),\n",
       "        Interval(role='precursor', length='15d', gap='15d')\n",
       "    ]\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check calendar\n",
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_folder = '~/AI4S2S/data'\n",
    "precursor_field = xr.open_dataset(Path(data_folder, 'sst_daily_1979-2018_5deg_Pacific_175_240E_25_50N.nc'))\n",
    "target_field = xr.open_dataset(Path(data_folder,'tf5_nc5_dendo_80d77.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the calendar to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map calendar to data\n",
    "calendar.map_to_data(precursor_field)\n",
    "calendar.visualize(show_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>i_interval</th>\n",
       "      <th>-6</th>\n",
       "      <th>-5</th>\n",
       "      <th>-4</th>\n",
       "      <th>-3</th>\n",
       "      <th>-2</th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>[2018-02-02, 2018-02-17)</td>\n",
       "      <td>[2018-03-04, 2018-03-19)</td>\n",
       "      <td>[2018-04-03, 2018-04-18)</td>\n",
       "      <td>[2018-05-03, 2018-05-18)</td>\n",
       "      <td>[2018-06-02, 2018-06-17)</td>\n",
       "      <td>[2018-07-02, 2018-07-17)</td>\n",
       "      <td>[2018-08-01, 2018-08-31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>[2017-02-02, 2017-02-17)</td>\n",
       "      <td>[2017-03-04, 2017-03-19)</td>\n",
       "      <td>[2017-04-03, 2017-04-18)</td>\n",
       "      <td>[2017-05-03, 2017-05-18)</td>\n",
       "      <td>[2017-06-02, 2017-06-17)</td>\n",
       "      <td>[2017-07-02, 2017-07-17)</td>\n",
       "      <td>[2017-08-01, 2017-08-31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>[2016-02-03, 2016-02-18)</td>\n",
       "      <td>[2016-03-04, 2016-03-19)</td>\n",
       "      <td>[2016-04-03, 2016-04-18)</td>\n",
       "      <td>[2016-05-03, 2016-05-18)</td>\n",
       "      <td>[2016-06-02, 2016-06-17)</td>\n",
       "      <td>[2016-07-02, 2016-07-17)</td>\n",
       "      <td>[2016-08-01, 2016-08-31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>[2015-02-02, 2015-02-17)</td>\n",
       "      <td>[2015-03-04, 2015-03-19)</td>\n",
       "      <td>[2015-04-03, 2015-04-18)</td>\n",
       "      <td>[2015-05-03, 2015-05-18)</td>\n",
       "      <td>[2015-06-02, 2015-06-17)</td>\n",
       "      <td>[2015-07-02, 2015-07-17)</td>\n",
       "      <td>[2015-08-01, 2015-08-31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>[2014-02-02, 2014-02-17)</td>\n",
       "      <td>[2014-03-04, 2014-03-19)</td>\n",
       "      <td>[2014-04-03, 2014-04-18)</td>\n",
       "      <td>[2014-05-03, 2014-05-18)</td>\n",
       "      <td>[2014-06-02, 2014-06-17)</td>\n",
       "      <td>[2014-07-02, 2014-07-17)</td>\n",
       "      <td>[2014-08-01, 2014-08-31)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "i_interval                         -6                        -5  \\\n",
       "anchor_year                                                       \n",
       "2018         [2018-02-02, 2018-02-17)  [2018-03-04, 2018-03-19)   \n",
       "2017         [2017-02-02, 2017-02-17)  [2017-03-04, 2017-03-19)   \n",
       "2016         [2016-02-03, 2016-02-18)  [2016-03-04, 2016-03-19)   \n",
       "2015         [2015-02-02, 2015-02-17)  [2015-03-04, 2015-03-19)   \n",
       "2014         [2014-02-02, 2014-02-17)  [2014-03-04, 2014-03-19)   \n",
       "\n",
       "i_interval                         -4                        -3  \\\n",
       "anchor_year                                                       \n",
       "2018         [2018-04-03, 2018-04-18)  [2018-05-03, 2018-05-18)   \n",
       "2017         [2017-04-03, 2017-04-18)  [2017-05-03, 2017-05-18)   \n",
       "2016         [2016-04-03, 2016-04-18)  [2016-05-03, 2016-05-18)   \n",
       "2015         [2015-04-03, 2015-04-18)  [2015-05-03, 2015-05-18)   \n",
       "2014         [2014-04-03, 2014-04-18)  [2014-05-03, 2014-05-18)   \n",
       "\n",
       "i_interval                         -2                        -1  \\\n",
       "anchor_year                                                       \n",
       "2018         [2018-06-02, 2018-06-17)  [2018-07-02, 2018-07-17)   \n",
       "2017         [2017-06-02, 2017-06-17)  [2017-07-02, 2017-07-17)   \n",
       "2016         [2016-06-02, 2016-06-17)  [2016-07-02, 2016-07-17)   \n",
       "2015         [2015-06-02, 2015-06-17)  [2015-07-02, 2015-07-17)   \n",
       "2014         [2014-06-02, 2014-06-17)  [2014-07-02, 2014-07-17)   \n",
       "\n",
       "i_interval                          1  \n",
       "anchor_year                            \n",
       "2018         [2018-08-01, 2018-08-31)  \n",
       "2017         [2017-08-01, 2017-08-31)  \n",
       "2016         [2016-08-01, 2016-08-31)  \n",
       "2015         [2015-08-01, 2015-08-31)  \n",
       "2014         [2014-08-01, 2014-08-31)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.show()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-validate-test split based on the anchor years (70%/15%/15% split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 70% of instance as training\n",
    "years = sorted(calendar.get_intervals().index)\n",
    "train_samples = round(len(years) * 0.7)\n",
    "test_samples = round(len(years) * 0.15)\n",
    "start_year = years[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit preprocessor with training samples and preprocess data\n",
    "Remove trend and take anomalies for the precursor field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessor\n",
    "preprocessor = preprocess.Preprocessor(\n",
    "    rolling_window_size=25,\n",
    "    detrend=\"linear\",\n",
    "    subtract_climatology=True,\n",
    ")\n",
    "\n",
    "# fit preprocessor with training data\n",
    "preprocessor.fit(precursor_field.sel(time=slice(str(start_year),\n",
    "                                                str(start_year + train_samples - 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the whole precursor field\n",
    "precursor_field_prep = preprocessor.transform(precursor_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample data to the calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursor_field_resample = lilio.resample(calendar, precursor_field_prep)\n",
    "target_field_resample = lilio.resample(calendar, target_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables and intervals\n",
    "precursor_field_sel = precursor_field_resample['sst']\n",
    "target_series_sel = target_field_resample['ts'].sel(cluster=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice and reshape input desired by transformer\n",
    "sequence_precursor = len(precursor_field_sel.i_interval) - 1 # we only take precursor parts of i intervals\n",
    "lat_precursor = len(precursor_field_sel.latitude) - 1 # selection made for transformer desired dimension\n",
    "lon_precursor = len(precursor_field_sel.longitude) - 1 # same as above\n",
    "\n",
    "X_torch = torch.from_numpy(precursor_field_sel[:,:-1,:-1,:-1].data).type(torch.FloatTensor)\n",
    "y_torch = torch.from_numpy(target_series_sel[:,-1].data).type(torch.FloatTensor)\n",
    "\n",
    "X_torch = X_torch.view(-1, sequence_precursor, lat_precursor*lon_precursor)\n",
    "y_torch = y_torch.unsqueeze(1).unsqueeze(1).repeat(1, 1, lat_precursor*lon_precursor)\n",
    "\n",
    "# turn nan to 0.0\n",
    "X_torch = torch.nan_to_num(X_torch, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validate/test split and use pytorch dataloader\n",
    "train_X_torch = X_torch[:train_samples]\n",
    "train_y_torch = y_torch[:train_samples]\n",
    "\n",
    "valid_X_torch = X_torch[train_samples:train_samples + test_samples]\n",
    "valid_y_torch = y_torch[train_samples:train_samples + test_samples]\n",
    "\n",
    "test_X_torch = X_torch[-test_samples:]\n",
    "test_y_torch = y_torch[-test_samples:]\n",
    "\n",
    "# pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(train_X_torch, train_y_torch)\n",
    "valid_set = torch.utils.data.TensorDataset(valid_X_torch, valid_y_torch)\n",
    "test_set = torch.utils.data.TensorDataset(test_X_torch, test_y_torch)\n",
    "\n",
    "# data loader\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_set, batch_size = config.batch_size, shuffle = False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tansformer with multihead attention using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)\n",
    "     \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\n",
    "     \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional embedding\n",
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim / dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048,\n",
    "                 activation: nn.Module = nn.ReLU()) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        activation,\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward, activation),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout, activation)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward, activation),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "        return self.feed_forward(tgt)\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout, activation)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning with W&B\n",
    "System info and syncronize training information with W&B server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version 1.12.1\n",
      "Is CUDA available? False\n",
      "Device to be used for computation: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgit-yang\u001b[0m (\u001b[33mai4s2s\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))\n",
    "# call weights & biases service\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and initialize config for wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    epoch = 10,\n",
    "    num_encoder_layers = 1,\n",
    "    num_decoder_layers = 1,\n",
    "    dim_model = 48,\n",
    "    num_heads = 2,\n",
    "    dim_feedforward = 12,\n",
    "    batch_size = 4,\n",
    "    dropout = 0.1,\n",
    "    learning_rate = 0.001,\n",
    "    dataset = 'Weather',\n",
    "    architecture = 'Transformer'\n",
    ")\n",
    "\n",
    "# initialize weights & biases service\n",
    "#mode = 'online'\n",
    "mode = 'disabled'\n",
    "wandb.init(config=hyperparameters, project='geometric-shapes', entity='dianna-ai', mode=mode)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader and use batch \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size = config.batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "Initialize model and choose loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details:\n",
      " Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (attention): Residual(\n",
      "          (sublayer): MultiHeadAttention(\n",
      "            (heads): ModuleList(\n",
      "              (0): AttentionHead(\n",
      "                (q): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (k): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (v): Linear(in_features=48, out_features=24, bias=True)\n",
      "              )\n",
      "              (1): AttentionHead(\n",
      "                (q): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (k): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (v): Linear(in_features=48, out_features=24, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (linear): Linear(in_features=48, out_features=48, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): Residual(\n",
      "          (sublayer): Sequential(\n",
      "            (0): Linear(in_features=48, out_features=12, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=12, out_features=48, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (attention_1): Residual(\n",
      "          (sublayer): MultiHeadAttention(\n",
      "            (heads): ModuleList(\n",
      "              (0): AttentionHead(\n",
      "                (q): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (k): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (v): Linear(in_features=48, out_features=24, bias=True)\n",
      "              )\n",
      "              (1): AttentionHead(\n",
      "                (q): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (k): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (v): Linear(in_features=48, out_features=24, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (linear): Linear(in_features=48, out_features=48, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attention_2): Residual(\n",
      "          (sublayer): MultiHeadAttention(\n",
      "            (heads): ModuleList(\n",
      "              (0): AttentionHead(\n",
      "                (q): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (k): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (v): Linear(in_features=48, out_features=24, bias=True)\n",
      "              )\n",
      "              (1): AttentionHead(\n",
      "                (q): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (k): Linear(in_features=48, out_features=24, bias=True)\n",
      "                (v): Linear(in_features=48, out_features=24, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (linear): Linear(in_features=48, out_features=48, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): Residual(\n",
      "          (sublayer): Sequential(\n",
      "            (0): Linear(in_features=48, out_features=12, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=12, out_features=48, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=48, out_features=48, bias=True)\n",
      "  )\n",
      ")\n",
      "Optimizer details:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = Transformer(num_encoder_layers = hyperparameters[\"num_encoder_layers\"],\n",
    "                    num_decoder_layers = hyperparameters[\"num_decoder_layers\"],\n",
    "                    dim_model = hyperparameters[\"dim_model\"], \n",
    "                    num_heads = hyperparameters[\"num_heads\"], \n",
    "                    dim_feedforward = hyperparameters[\"dim_feedforward\"], \n",
    "                    dropout = hyperparameters[\"dropout\"])\n",
    "# Specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Choose optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "# Print model and optimizer details\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/28(0%)]\tLoss: 2.225106\n",
      "Epoch : 1 [0/28(0%)]\tLoss: 2.216071\n",
      "Epoch : 2 [0/28(0%)]\tLoss: 2.211614\n",
      "Epoch : 3 [0/28(0%)]\tLoss: 2.208896\n",
      "Epoch : 4 [0/28(0%)]\tLoss: 2.207437\n",
      "Epoch : 5 [0/28(0%)]\tLoss: 2.206661\n",
      "Epoch : 6 [0/28(0%)]\tLoss: 2.206189\n",
      "Epoch : 7 [0/28(0%)]\tLoss: 2.205882\n",
      "Epoch : 8 [0/28(0%)]\tLoss: 2.205631\n",
      "Epoch : 9 [0/28(0%)]\tLoss: 2.205462\n"
     ]
    }
   ],
   "source": [
    "# switch model into train mode\n",
    "model.train()\n",
    "\n",
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "\n",
    "for epoch in range(config.epoch):\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch).to(device)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(var_X_batch, var_y_batch)\n",
    "        loss = criterion(output, var_y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({'train_loss': loss.item()})\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Epoch : {epoch} [{batch_idx*len(X_batch)}/{len(train_loader.dataset)}'\n",
    "                  f'({100.* batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    # add validation results\n",
    "    # for batch_idx, (X_batch, y_batch) in enumerate(valid_loader):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2spy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
