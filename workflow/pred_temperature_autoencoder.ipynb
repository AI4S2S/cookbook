{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict 2 meter temperature with sea surface temperature using autoencoder with multi-head attention\n",
    "This notebook serves as an example of a basic workflow of data driven forecasting using deep learning with `s2spy` & `lilio` packages. <br>\n",
    "We will predict temperature in US at seasonal time scales using ERA5 dataset with multi-head attention autoencoder. <br>\n",
    "\n",
    "<img src=\"../assets/concept_test_case.png\" alt=\"usecase\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe includes the following steps:\n",
    "- Define a calendar (`lilio`)\n",
    "- Download/load input data (test data, accessible via `era5cli`)\n",
    "- Map the calendar to the data (`lilio`)\n",
    "- Train-validate-test split (60%/20%/20%)\n",
    "- Preprocessing based on the training set (`s2spy`)\n",
    "- Resample data to the calendar (`lilio`)\n",
    "- Create autoencoder model (`torch`)\n",
    "- Specify hyper-parameters (`wandb`)\n",
    "- Train model (`torch`)\n",
    "- Evaludate model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow is illustrated below:\n",
    "\n",
    "<img src=\"../assets/dl.PNG\" alt=\"Transformer\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lilio\n",
    "import numpy as np\n",
    "import sys\n",
    "import time as tt\n",
    "import wandb\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from s2spy import preprocess\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as f\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "from autoencoder import Transformer\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a calendar with `lilio` to specify time range for targets and precursors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom calendar based on the time of interest\n",
    "calendar = lilio.Calendar(anchor=\"07-01\", allow_overlap=True)\n",
    "# add target periods\n",
    "calendar.add_intervals(\"target\", length=\"30d\", gap=\"1M\")\n",
    "# add precursor periods\n",
    "periods_of_interest = 8\n",
    "calendar.add_intervals(\"precursor\", \"1M\", n=periods_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check calendar\n",
    "calendar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test data SST and (clustered) T2M\n",
    "For the sake of batch size, we use 61 years (1961-2021) of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "precursor_field = xr.open_dataset('../data/sst_daily_1959-2021_5deg_Pacific_175_240E_25_50N.nc')\n",
    "precursor_field = precursor_field.sel(time=slice(\"19610101\",\"20211231\"))\n",
    "target_field = xr.open_dataset('../data/t2m_daily_1959-2021_2deg_clustered_226_300E_30_70N.nc')\n",
    "target_field = target_field.sel(time=slice(\"19610101\",\"20211231\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Klevin to Celcius\n",
    "precursor_field[\"sst\"] = precursor_field[\"sst\"] - 273.15\n",
    "target_field[\"t2m\"] = target_field[\"t2m\"] - 273.15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the calendar to the data\n",
    "After mapping the calendar to the field, we can visualize our calendar by calling the `visualize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map calendar to data\n",
    "calendar.map_to_data(precursor_field)\n",
    "calendar.visualize(show_length=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can get a list of all intervals by running the following line. There, you will find the intervals `-1` and `1`, which corresponds to the creation of a precursor interval (negative integer(s)) and a target interval (positive integer(s)), respectively. <br>\n",
    "\n",
    "For more information about the definition of intervals, and how `lilio` works, please check the [README](https://github.com/AI4S2S/lilio) of `lilio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.show()[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-validate-test split based on the anchor years (60%/20%/20% split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 60% of instance as training\n",
    "years = sorted(calendar.get_intervals().index)\n",
    "train_samples = round(len(years) * 0.6)\n",
    "test_samples = round(len(years) * 0.2)\n",
    "start_year = years[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit preprocessor with training samples and preprocess data\n",
    "Remove trend and take anomalies for the precursor field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessor\n",
    "preprocessor = preprocess.Preprocessor(\n",
    "    rolling_window_size=25,\n",
    "    detrend=\"linear\",\n",
    "    subtract_climatology=True,\n",
    ")\n",
    "\n",
    "# fit preprocessor with training data\n",
    "preprocessor.fit(\n",
    "    precursor_field.sel(\n",
    "        time=slice(str(start_year), str(start_year + train_samples - 1))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the whole precursor field\n",
    "precursor_field_prep = preprocessor.transform(precursor_field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample data to the calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursor_field_resample = lilio.resample(calendar, precursor_field_prep)\n",
    "target_field_resample = lilio.resample(calendar, target_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables and intervals\n",
    "precursor_field_sel = precursor_field_resample['sst']\n",
    "target_series_sel = target_field_resample['t2m'].sel(cluster=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice and reshape input desired by transformer\n",
    "sequence_precursor = len(precursor_field_sel.i_interval) - 1 # we only take precursor parts of i intervals\n",
    "lat_precursor = len(precursor_field_sel.latitude)\n",
    "lon_precursor = len(precursor_field_sel.longitude)\n",
    "\n",
    "X_torch = torch.from_numpy(precursor_field_sel[:,:-1,:,:].data).type(torch.FloatTensor)\n",
    "y_torch = torch.from_numpy(target_series_sel[:,-1].data).type(torch.FloatTensor)\n",
    "\n",
    "X_torch = X_torch.view(-1, sequence_precursor, lat_precursor*lon_precursor)\n",
    "\n",
    "# turn nan to 0.0\n",
    "X_torch = torch.nan_to_num(X_torch, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validate/test split and use pytorch dataloader\n",
    "train_X_torch = X_torch[:train_samples]\n",
    "train_y_torch = y_torch[:train_samples]\n",
    "\n",
    "valid_X_torch = X_torch[train_samples:train_samples + test_samples]\n",
    "valid_y_torch = y_torch[train_samples:train_samples + test_samples]\n",
    "\n",
    "test_X_torch = X_torch[-test_samples:]\n",
    "test_y_torch = y_torch[-test_samples:]\n",
    "\n",
    "# pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(train_X_torch, train_y_torch)\n",
    "valid_set = torch.utils.data.TensorDataset(valid_X_torch, valid_y_torch)\n",
    "test_set = torch.utils.data.TensorDataset(test_X_torch, test_y_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning with W&B\n",
    "System info and syncronize training information with W&B server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and track your runs with Weights and Biases (wandb) service. You'll need an account, a team, and a project if you'll want to track runs online. Otherwise, you can simply run the code by setting mode = 'disabled' (W&B will not be active). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    epoch = 150,\n",
    "    num_encoder_layers = 1,\n",
    "    dim_model = lat_precursor*lon_precursor,\n",
    "    num_heads = 2,\n",
    "    dim_feedforward = 12,\n",
    "    dim_output = 1,\n",
    "    batch_size = 4,\n",
    "    dropout = 0.1,\n",
    "    learning_rate = 0.01,\n",
    "    periods_of_interest = periods_of_interest,\n",
    "    dataset = 'Weather',\n",
    "    architecture = 'Transformer'\n",
    ")\n",
    "\n",
    "# call weights & biases service\n",
    "wandb.login()\n",
    "\n",
    "# initialize weights & biases service\n",
    "mode = 'disabled'\n",
    "# mode = 'online' # <- uncomment this line to enable wandb\n",
    "team = 'ai4s2s-demo' # <- your own team namehere\n",
    "project = 'test-autoencoder' # <- your own project name here\n",
    "wandb.init(config=hyperparameters, project=project, entity=team, mode=mode)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader and use batch \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size = config.batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and train model\n",
    "Create autoencoder. Initialize model and choose loss function and optimizer. <br>\n",
    "\n",
    "The architecture of the autoencoder used here is shown in the figure below. This structure is very similar to the famous language model called BERT. For more details about the full transformer network structure, check the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "<img src=\"../assets/bert.png\" alt=\"BERT\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Transformer(num_encoder_layers = config[\"num_encoder_layers\"],\n",
    "                    dim_model = config[\"dim_model\"], \n",
    "                    num_heads = config[\"num_heads\"], \n",
    "                    dim_feedforward = config[\"dim_feedforward\"], \n",
    "                    dim_output = config[\"dim_output\"], \n",
    "                    dropout = config[\"dropout\"])\n",
    "# Specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Choose optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "# Print model and optimizer details\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the total number of parameters\n",
    "utils.total_num_param(model)\n",
    "# for more details about the trainable parameter in each layer\n",
    "#utils.param_trainable(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "\n",
    "# switch model into training mode\n",
    "model.train()\n",
    "\n",
    "hist_train = []\n",
    "hist_valid = []\n",
    "for epoch in range(config.epoch):\n",
    "    # training loop\n",
    "    # switch model into train mode\n",
    "    model.train()\n",
    "    hist_train_step = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch).to(device)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(var_X_batch)\n",
    "        loss = criterion(output.squeeze(), var_y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({'train_loss': loss.item()})\n",
    "        if batch_idx % 2 == 0:\n",
    "            print(f'Epoch : {epoch} [{batch_idx*len(X_batch)}/{len(train_loader.dataset)}'\n",
    "                  f'({100.* batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        hist_train_step += loss.item()\n",
    "\n",
    "    hist_train.append(hist_train_step / len(train_loader.dataset))\n",
    "\n",
    "    # cross-validation loop\n",
    "    # switch model into evaluation mode\n",
    "    model.eval()\n",
    "    hist_valid_step = 0\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(valid_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch).to(device)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            output = model(var_X_batch)\n",
    "            loss = criterion(output.squeeze(), var_y_batch)\n",
    "        wandb.log({'validation_loss': loss.item()})\n",
    "        hist_valid_step += loss.item()\n",
    "\n",
    "    hist_valid.append(hist_valid_step / len(valid_loader.dataset))\n",
    "\n",
    "print (f\"--- {(tt.time() - start_time)/60} minutes ---\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the training loss and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(np.asarray(hist_train), 'b', label=\"Training loss\")\n",
    "plt.plot(np.asarray(hist_valid), 'r', label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(np.log(np.asarray(hist_train)), 'b', label=\"Training loss\")\n",
    "plt.plot(np.log(np.asarray(hist_valid)), 'r', label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Logarithmic loss')\n",
    "plt.title(\"Logarithmic MSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the checkpoint model training\n",
    "output_path = \"./\"\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, Path(output_path,'autoencoder_train_checkpoint.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "Now we can evaluate our model with testing set and compare the predictions with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch model into evaluation mode\n",
    "model.eval()\n",
    "hist_test = []\n",
    "predictions = []\n",
    "hist_test_step = 0\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(test_loader):\n",
    "    var_X_batch = torch.autograd.Variable(X_batch).to(device)\n",
    "    var_y_batch = torch.autograd.Variable(y_batch).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        output = model(var_X_batch)\n",
    "        loss = criterion(output.squeeze(), var_y_batch)\n",
    "    wandb.log({'testing_loss': loss.item()})\n",
    "    predictions.append(output.squeeze().cpu().detach().numpy())\n",
    "    hist_test_step += loss.item()\n",
    "\n",
    "hist_test.append(hist_test_step / len(test_loader.dataset))\n",
    "# call wandb finish to stop logging\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The MSE loss is {hist_test[0]:.3f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "instances = np.arange(len(np.concatenate(predictions)))\n",
    "plt.scatter(instances, np.concatenate(predictions), label=\"Predictions\")\n",
    "plt.scatter(instances, test_y_torch.numpy(), label=\"Ground truth\")\n",
    "plt.xlabel(\"Experiment\")\n",
    "plt.ylabel(\"TS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2spy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
