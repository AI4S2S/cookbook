{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sea ice concentration with sea surface flux using transformer with multi-head attention\n",
    "This notebook serves as an example of a basic workflow with `s2spy` & `lilio` packages. <br>\n",
    "We will predict sea ice concentration in the Arctic at subseasonal time scales using ERA5 dataset with multi-head attention transformer. <br>\n",
    "\n",
    "This recipe includes the following steps:\n",
    "- Define a calendar (`lilio`)\n",
    "- Download/load input data\n",
    "- Map the calendar to the data (`lilio`)\n",
    "- Train-test split (70%/30%) (`torch`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "import lilio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a calendar with `lilio` to specify time range for targets and precursors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NWP calendar for constrained forecasts/hindcasts\n",
    "# create custom calendar based on the time of interest\n",
    "calendar = lilio.Calendar(anchor=\"01-01\", allow_overlap=True)\n",
    "# add target periods\n",
    "# determine number of weeks\n",
    "w_start = date(1979, 1, 1)\n",
    "w_end = date(2017, 12, 31)\n",
    "days = (w_end - w_start).days\n",
    "weeks = days // 7\n",
    "# add target periods\n",
    "for _ in range(weeks):\n",
    "    calendar.add_intervals(\"target\", length=\"7d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_folder = '~/AI4S2S/data'\n",
    "precursor_field = xr.open_dataset(Path(data_folder, 'rad_daily'))\n",
    "target_field = xr.open_dataset(Path(data_folder,'sic_daily'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the calendar to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map calendar to data\n",
    "calendar.map_to_data(precursor_field)\n",
    "calendar.visualize(show_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 70% of instance as training\n",
    "years = calendar.get_intervals().index\n",
    "train_samples = round(len(years) * 0.7)\n",
    "start_year = years[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit preprocessor with training samples and preprocess data\n",
    "Remove trend and take anomalies for the precursor field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split based on the anchor years (70%/30% split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tansformer with multihead attention using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)\n",
    "     \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\n",
    "     \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning with W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "Implement early stop function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2spy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
