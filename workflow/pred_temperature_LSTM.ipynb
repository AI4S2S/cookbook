{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict 2 meter temperature with sea surface temperature using LSTM\n",
    "This notebook serves as an example of a basic workflow of data driven forecasting using deep learning with `s2spy` & `lilio` packages. <br>\n",
    "We will predict temperature in US at seasonal time scales using ERA5 dataset with LSTM network. <br>\n",
    "\n",
    "<img src=\"../assets/concept_test_case.png\" alt=\"usecase\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe includes the following steps:\n",
    "- Define a calendar (`lilio`)\n",
    "- Download/load input data (`era5cli`) (test data, accessible via `era5cli`)\n",
    "- Map the calendar to the data (`lilio`)\n",
    "- Train-validate-test split (60%/20%/20%)\n",
    "- Preprocessing based on the training set (`s2spy`)\n",
    "- Resample data to the calendar (`lilio`)\n",
    "- Create LSTM model (`torch`)\n",
    "- Specify hyper-parameters (`wandb`)\n",
    "- Train model (`torch`)\n",
    "- Evaludate model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow is illustrated below:\n",
    "\n",
    "<img src=\"../assets/dl.PNG\" alt=\"Transformer\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lilio\n",
    "import numpy as np\n",
    "import time as tt\n",
    "import wandb\n",
    "import sys\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from s2spy import preprocess\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# import utils function to check the statistics of parameters\n",
    "sys.path.append(\"../src/\")\n",
    "import utils\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a calendar with `lilio` to specify time range for targets and precursors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom calendar based on the time of interest\n",
    "calendar = lilio.Calendar(anchor=\"07-01\", allow_overlap=True)\n",
    "# add target periods\n",
    "calendar.add_intervals(\"target\", length=\"30d\", gap=\"1M\")\n",
    "# add precursor periods\n",
    "periods_of_interest = 8\n",
    "calendar.add_intervals(\"precursor\", \"1M\", n=periods_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check calendar\n",
    "calendar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test data SST and (clustered) T2M\n",
    "For the sake of batch size, we use 61 years (1961-2021) of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "precursor_field = xr.open_dataset('../data/sst_daily_1959-2021_5deg_Pacific_175_240E_25_50N.nc')\n",
    "precursor_field = precursor_field.sel(time=slice(\"19610101\",\"20211231\"))\n",
    "target_field = xr.open_dataset('../data/t2m_daily_1959-2021_2deg_clustered_226_300E_30_70N.nc')\n",
    "target_field = target_field.sel(time=slice(\"19610101\",\"20211231\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Klevin to Celcius\n",
    "precursor_field[\"sst\"] = precursor_field[\"sst\"] - 273.15\n",
    "target_field[\"t2m\"] = target_field[\"t2m\"] - 273.15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the calendar to the data\n",
    "After mapping the calendar to the field, we can visualize our calendar by calling the `visualize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map calendar to data\n",
    "calendar.map_to_data(precursor_field)\n",
    "calendar.visualize(show_length=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can get a list of all intervals by running the following line. There, you will find the intervals `-1` and `1`, which corresponds to the creation of a precursor interval (negative integer(s)) and a target interval (positive integer(s)), respectively. <br>\n",
    "\n",
    "For more information about the definition of intervals, and how `lilio` works, please check the [README](https://github.com/AI4S2S/lilio) of `lilio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.show()[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-validate-test split based on the anchor years (60%/20%/20% split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 60% of instance as training\n",
    "years = sorted(calendar.get_intervals().index)\n",
    "train_samples = round(len(years) * 0.6)\n",
    "test_samples = round(len(years) * 0.2)\n",
    "start_year = years[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit preprocessor with training samples and preprocess data\n",
    "In this step, we remove trend and take anomalies for the precursor field. Note that here we use raw daily data for detrending and taking anomalies. <br>\n",
    "\n",
    "In general, there are many \"flavors\" of preprocessing, like when to perform this operation, and in which order do we want to preprocess the data. To improve the transparency and reproducibility of our work, we think it is necessary to standardize these steps. To stick to the best practices, we suggest to preprocess your data in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessor\n",
    "preprocessor = preprocess.Preprocessor(\n",
    "    rolling_window_size=25,\n",
    "    detrend=\"linear\",\n",
    "    subtract_climatology=True,\n",
    ")\n",
    "\n",
    "# fit preprocessor with training data\n",
    "preprocessor.fit(\n",
    "    precursor_field.sel(\n",
    "        time=slice(str(start_year), str(start_year + train_samples - 1))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the whole precursor field\n",
    "precursor_field_prep = preprocessor.transform(precursor_field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample data to the calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursor_field_resample = lilio.resample(calendar, precursor_field_prep)\n",
    "target_field_resample = lilio.resample(calendar, target_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables and intervals\n",
    "precursor_field_sel = precursor_field_resample['sst']\n",
    "target_series_sel = target_field_resample['t2m'].sel(cluster=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our data to `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice and reshape input desired by transformer\n",
    "sequence_precursor = len(precursor_field_sel.i_interval) - 1 # we only take precursor parts of i intervals\n",
    "lat_precursor = len(precursor_field_sel.latitude)\n",
    "lon_precursor = len(precursor_field_sel.longitude)\n",
    "\n",
    "X_torch = torch.from_numpy(precursor_field_sel[:,:-1,:,:].data).type(torch.FloatTensor)\n",
    "y_torch = torch.from_numpy(target_series_sel[:,-1].data).type(torch.FloatTensor)\n",
    "\n",
    "X_torch = X_torch.view(-1, sequence_precursor, lat_precursor*lon_precursor)\n",
    "\n",
    "# turn nan to 0.0\n",
    "X_torch = torch.nan_to_num(X_torch, 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into train/cross-validate/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validate/test split and use pytorch dataloader\n",
    "train_X_torch = X_torch[:train_samples]\n",
    "train_y_torch = y_torch[:train_samples]\n",
    "\n",
    "valid_X_torch = X_torch[train_samples:train_samples + test_samples]\n",
    "valid_y_torch = y_torch[train_samples:train_samples + test_samples]\n",
    "\n",
    "test_X_torch = X_torch[-test_samples:]\n",
    "test_y_torch = y_torch[-test_samples:]\n",
    "\n",
    "# pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(train_X_torch, train_y_torch)\n",
    "valid_set = torch.utils.data.TensorDataset(valid_X_torch, valid_y_torch)\n",
    "test_set = torch.utils.data.TensorDataset(test_X_torch, test_y_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LSTM model\n",
    "Build a LSTM model with `nn.LSTM` module.\n",
    "\n",
    "The architecture of the autoencoder used here is shown in the figure below.\n",
    "\n",
    "<img src=\"../assets/lstm.png\" alt=\"LSTM\" width=\"500\"/>\n",
    "\n",
    "(source of image: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1,\n",
    "                 batch_size=1, num_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model in Pytorch and specify the basic model structure.\n",
    "        Expected input timeseries dimension [batch_size, sequence, channels]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers, batch_first = True, dropout = dropout)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"Initialize hidden state with random values.\"\"\"\n",
    "        return (torch.randn(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.randn(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        (h_0, c_0) = self.init_hidden()\n",
    "        x, _ = self.lstm(input, (h_0, c_0))\n",
    "        x = self.linear(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning with W&B\n",
    "We use Weight&Biases to monitor the training process. It is very simple to integrate it into our workflow and more information about how to set it up can be found at https://docs.wandb.ai/quickstart. <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print system info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and track your runs with Weights and Biases (wandb) service. You'll need an account, a team, and a project if you'll want to track runs online. Otherwise, you can simply run the code by setting mode = 'disabled' (W&B will not be active). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters and the \n",
    "hyperparameters = dict(\n",
    "    epoch = 150,\n",
    "    input_dim = lat_precursor*lon_precursor,\n",
    "    hidden_dim = lat_precursor*lon_precursor*2,\n",
    "    output_dim = 1,\n",
    "    batch_size = 4, \n",
    "    num_layers = 2,\n",
    "    dropout = 0.0,\n",
    "    learning_rate = 0.02,\n",
    "    dataset = 'Weather',\n",
    "    architecture = 'LSTM'\n",
    ")\n",
    "\n",
    "# call weights & biases service\n",
    "wandb.login()\n",
    "\n",
    "# initialize weights & biases service\n",
    "mode = 'disabled'\n",
    "# mode = 'online' # <- uncomment this line to enable wandb\n",
    "team = 'ai4s2s-demo' # <- your own team namehere\n",
    "project = 'test-LSTM' # <- your own project name here\n",
    "wandb.init(config=hyperparameters, project=project, entity=team, mode=mode)\n",
    "config = wandb.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders with chosen batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader and use batch \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size = config.batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and train model\n",
    "Create model using specified hyperparameter. Initialize model and choose loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LSTM(input_dim = config[\"input_dim\"],\n",
    "             hidden_dim = config[\"hidden_dim\"],\n",
    "             output_dim = config[\"output_dim\"], \n",
    "             batch_size = config[\"batch_size\"], \n",
    "             num_layers = config[\"num_layers\"]\n",
    ")\n",
    "# Specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Choose optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "# Print model and optimizer details\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the total number of parameters\n",
    "utils.total_num_param(model)\n",
    "# for more details about the trainable parameter in each layer\n",
    "#utils.param_trainable(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training and cross validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "\n",
    "# switch model into training mode\n",
    "model.train()\n",
    "\n",
    "hist_train = []\n",
    "hist_valid = []\n",
    "for epoch in range(config.epoch):\n",
    "    # training loop\n",
    "    # switch model into train mode\n",
    "    model.train()\n",
    "    hist_train_step = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        var_X_batch = Variable(X_batch).to(device)\n",
    "        var_y_batch = Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # note: decoder input is the last instance of encoder input\n",
    "        output = model(var_X_batch)\n",
    "        loss = criterion(output[:,-1,:].squeeze(), var_y_batch) # we only need the last instance from output sequence\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({'train_loss': loss.item()})\n",
    "        print(f'Epoch : {epoch} [{batch_idx*len(X_batch)}/{len(train_loader.dataset)}'\n",
    "              f'({100.* batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        hist_train_step += loss.item()\n",
    "\n",
    "    hist_train.append(hist_train_step / len(train_loader.dataset))\n",
    "\n",
    "    # cross-validation loop\n",
    "    # switch model into evaluation mode\n",
    "    model.eval()\n",
    "    hist_valid_step = 0\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(valid_loader):\n",
    "        var_X_batch = Variable(X_batch).to(device)\n",
    "        var_y_batch = Variable(y_batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            output = model(var_X_batch)\n",
    "            loss = criterion(output[:,-1,:].squeeze(), var_y_batch)\n",
    "        wandb.log({'validation_loss': loss.item()})\n",
    "        hist_valid_step += loss.item()\n",
    "\n",
    "    hist_valid.append(hist_valid_step / len(valid_loader.dataset))\n",
    "\n",
    "print (f\"--- {(tt.time() - start_time)/60} minutes ---\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the training loss and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(np.asarray(hist_train), 'b', label=\"Training loss\")\n",
    "plt.plot(np.asarray(hist_valid), 'r', label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.semilogy(np.asarray(hist_train), 'b', label=\"Training loss\")\n",
    "plt.semilogy(np.asarray(hist_valid), 'r', label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Logarithmic loss')\n",
    "plt.title(\"Logarithmic MSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the checkpoint model training if necessary\n",
    "output_path = \"./\"\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, Path(output_path,'lstm_train_checkpoint.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "Now we can evaluate our model with testing set and compare the predictions with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch model into evaluation mode\n",
    "model.eval()\n",
    "hist_test = []\n",
    "predictions = []\n",
    "hist_test_step = 0\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(test_loader):\n",
    "    var_X_batch = Variable(X_batch).to(device)\n",
    "    var_y_batch = Variable(y_batch).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        output = model(var_X_batch)\n",
    "        loss = criterion(output[:,-1,:].squeeze(), var_y_batch)\n",
    "    wandb.log({'testing_loss': loss.item()})\n",
    "    predictions.append(output.squeeze().cpu().detach().numpy()[:,-1])\n",
    "    hist_test_step += loss.item()\n",
    "\n",
    "hist_test.append(hist_test_step / len(test_loader.dataset))\n",
    "# call wandb finish to stop logging\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predictions versus ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The MSE loss is {hist_test[0]:.3f}\")\n",
    "\n",
    "fig = plt.figure()\n",
    "instances = np.arange(len(np.concatenate(predictions)))\n",
    "plt.scatter(instances, np.concatenate(predictions), label=\"Predictions\")\n",
    "plt.scatter(instances, test_y_torch.squeeze().numpy(), label=\"Ground truth\")\n",
    "plt.scatter(instances, [test_y_torch.squeeze().numpy().mean()] * len(instances), label=\"Mean of ground truth\")\n",
    "plt.xlabel(\"Experiment\")\n",
    "plt.ylabel(\"TS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2spy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
