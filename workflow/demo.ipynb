{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict 2 meter temperature with sea surface temperature using deep learning and evaluate the predictions with linear regression results\n",
    "\n",
    "![forecast](../assets/concept_example_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lilio\n",
    "import numpy as np\n",
    "import sys\n",
    "import time as tt\n",
    "import wandb\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from s2spy import preprocess\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "from transformer import Transformer\n",
    "import utils\n",
    "\n",
    "# for reproducibility \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare data for data-driven forecasting with `s2spy` & `lilio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom calendar based on the time of interest\n",
    "calendar = lilio.Calendar(anchor=\"08-01\", allow_overlap=True)\n",
    "# add target periods\n",
    "calendar.add_intervals(\"target\", length=\"20d\")\n",
    "# add precursor periods\n",
    "periods_of_interest = 6\n",
    "calendar.add_intervals(\"precursor\", \"20d\", gap=\"10d\", n=periods_of_interest)\n",
    "\n",
    "# load example data\n",
    "data_folder = '~/AI4S2S/data'\n",
    "precursor_field = xr.open_dataset(Path(data_folder, 'sst_daily_1979-2018_5deg_Pacific_175_240E_25_50N.nc'))\n",
    "target_field = xr.open_dataset(Path(data_folder,'tf5_nc5_dendo_80d77.nc'))\n",
    "\n",
    "# map calendar to data\n",
    "calendar.map_to_data(precursor_field)\n",
    "calendar.visualize(show_length=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-validate-test split based on the anchor years (70%/15%/15% split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 70% of instance as training\n",
    "years = sorted(calendar.get_intervals().index)\n",
    "train_samples = round(len(years) * 0.7)\n",
    "test_samples = round(len(years) * 0.15)\n",
    "start_year = years[0]\n",
    "\n",
    "# create preprocessor\n",
    "preprocessor = preprocess.Preprocessor(\n",
    "    rolling_window_size=25,\n",
    "    detrend=\"linear\",\n",
    "    subtract_climatology=True,\n",
    ")\n",
    "\n",
    "# fit preprocessor with training data\n",
    "preprocessor.fit(precursor_field.sel(time=slice(str(start_year),\n",
    "                                                str(start_year + train_samples - 1))))\n",
    "\n",
    "# preprocess the whole precursor field\n",
    "precursor_field_prep = preprocessor.transform(precursor_field)\n",
    "\n",
    "precursor_field_resample = lilio.resample(calendar, precursor_field_prep)\n",
    "target_field_resample = lilio.resample(calendar, target_field)\n",
    "\n",
    "# select variables and intervals\n",
    "precursor_field_sel = precursor_field_resample['sst']\n",
    "target_series_sel = target_field_resample['ts'].sel(cluster=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a deep neural network for forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice and reshape input desired by transformer\n",
    "sequence_precursor = len(precursor_field_sel.i_interval) - 1 # we only take precursor parts of i intervals\n",
    "lat_precursor = len(precursor_field_sel.latitude)\n",
    "lon_precursor = len(precursor_field_sel.longitude)\n",
    "\n",
    "X_torch = torch.from_numpy(precursor_field_sel[:,:-1,:,:].data).type(torch.FloatTensor)\n",
    "y_torch = torch.from_numpy(target_series_sel[:,-1].data).type(torch.FloatTensor)\n",
    "\n",
    "X_torch = X_torch.view(-1, sequence_precursor, lat_precursor*lon_precursor)\n",
    "y_torch = y_torch.unsqueeze(1).unsqueeze(1).repeat(1, 1, lat_precursor*lon_precursor)\n",
    "\n",
    "# turn nan to 0.0\n",
    "X_torch = torch.nan_to_num(X_torch, 0.0)\n",
    "\n",
    "# train/validate/test split and use pytorch dataloader\n",
    "train_X_torch = X_torch[:train_samples]\n",
    "train_y_torch = y_torch[:train_samples]\n",
    "\n",
    "valid_X_torch = X_torch[train_samples:train_samples + test_samples]\n",
    "valid_y_torch = y_torch[train_samples:train_samples + test_samples]\n",
    "\n",
    "test_X_torch = X_torch[-test_samples:]\n",
    "test_y_torch = y_torch[-test_samples:]\n",
    "\n",
    "# pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(train_X_torch, train_y_torch)\n",
    "valid_set = torch.utils.data.TensorDataset(valid_X_torch, valid_y_torch)\n",
    "test_set = torch.utils.data.TensorDataset(test_X_torch, test_y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call weights & biases service\n",
    "wandb.login()\n",
    "\n",
    "hyperparameters = dict(\n",
    "    epoch = 100,\n",
    "    num_encoder_layers = 1,\n",
    "    num_decoder_layers = 1,\n",
    "    dim_model = lat_precursor*lon_precursor,\n",
    "    num_heads = 2,\n",
    "    dim_feedforward = 12,\n",
    "    batch_size = 4,\n",
    "    dropout = 0.1,\n",
    "    learning_rate = 0.01,\n",
    "    periods_of_interest = periods_of_interest,\n",
    "    dataset = 'Weather',\n",
    "    architecture = 'Transformer'\n",
    ")\n",
    "\n",
    "# initialize weights & biases service\n",
    "mode = 'online'\n",
    "#mode = 'disabled'\n",
    "wandb.init(config=hyperparameters, project='demo', entity='ai4s2s', mode=mode)\n",
    "config = wandb.config\n",
    "\n",
    "# create data loader and use batch \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = config.batch_size, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size = config.batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = config.batch_size, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](../assets/transformer.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Transformer(num_encoder_layers = config[\"num_encoder_layers\"],\n",
    "                    num_decoder_layers = config[\"num_decoder_layers\"],\n",
    "                    dim_model = config[\"dim_model\"], \n",
    "                    num_heads = config[\"num_heads\"], \n",
    "                    dim_feedforward = config[\"dim_feedforward\"], \n",
    "                    dropout = config[\"dropout\"])\n",
    "# Specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Choose optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "# Print model and optimizer details\n",
    "print('Model details:\\n', model)\n",
    "print('Optimizer details:\\n',optimizer)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time for the code execution\n",
    "start_time = tt.time()\n",
    "\n",
    "# switch model into training mode\n",
    "model.train()\n",
    "\n",
    "hist_train = []\n",
    "hist_valid = []\n",
    "for epoch in range(config.epoch):\n",
    "    # training loop\n",
    "    # switch model into train mode\n",
    "    model.train()\n",
    "    hist_train_step = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        # note: decoder input is the last instance of encoder input\n",
    "        output = model(var_X_batch, var_X_batch.clone()[:,-1:,:])\n",
    "        loss = criterion(output, var_y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({'train_loss': loss.item()})\n",
    "        if batch_idx % 2 == 0:\n",
    "            print(f'Epoch : {epoch} [{batch_idx*len(X_batch)}/{len(train_loader.dataset)}'\n",
    "                  f'({100.* batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        hist_train_step += loss.item()\n",
    "\n",
    "    hist_train.append(hist_train_step / len(train_loader.dataset))\n",
    "\n",
    "    # cross-validation loop\n",
    "    # switch model into evaluation mode\n",
    "    model.eval()\n",
    "    hist_valid_step = 0\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(valid_loader):\n",
    "        var_X_batch = torch.autograd.Variable(X_batch)\n",
    "        var_y_batch = torch.autograd.Variable(y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            output = model(var_X_batch, var_X_batch.clone()[:,-1:,:])\n",
    "            loss = criterion(output, var_y_batch)\n",
    "        wandb.log({'validation_loss': loss.item()})\n",
    "        hist_valid_step += loss.item()\n",
    "\n",
    "    hist_valid.append(hist_valid_step / len(valid_loader.dataset))\n",
    "\n",
    "print (f\"--- {(tt.time() - start_time)/60} minutes ---\")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.asarray(hist_train), 'b', label=\"Training loss\")\n",
    "plt.plot(np.asarray(hist_valid), 'r', label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch model into evaluation mode\n",
    "model.eval()\n",
    "hist_test = []\n",
    "predictions = []\n",
    "hist_test_step = 0\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(test_loader):\n",
    "    var_X_batch = torch.autograd.Variable(X_batch)\n",
    "    var_y_batch = torch.autograd.Variable(y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        output = model(var_X_batch, var_X_batch.clone()[:,-1:,:])\n",
    "        loss = criterion(output, var_y_batch)\n",
    "    wandb.log({'testing_loss': loss.item()})\n",
    "    predictions.append(torch.mean(output.squeeze(), dim=1).cpu().detach().numpy())\n",
    "    hist_test_step += loss.item()\n",
    "\n",
    "hist_test.append(hist_test_step / len(test_loader.dataset))\n",
    "# call wandb finish to stop logging\n",
    "wandb.finish()\n",
    "\n",
    "fig = plt.figure()\n",
    "instances = np.arange(len(np.concatenate(predictions)))\n",
    "plt.scatter(instances, np.concatenate(predictions), label=\"Predictions\")\n",
    "plt.scatter(instances, np.mean(test_y_torch.squeeze().numpy(), 1), label=\"Ground truth\")\n",
    "plt.xlabel(\"Experiment\")\n",
    "plt.ylabel(\"TS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate predictions with baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s2spy import RGDR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# cross-validation with Kfold\n",
    "k_fold_splits = 5\n",
    "kfold = KFold(n_splits=k_fold_splits)\n",
    "cv = lilio.traintest.TrainTestSplit(kfold)\n",
    "\n",
    "# create lists for saving models and predictions\n",
    "models = []\n",
    "RGDRs = []\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "train_test_splits = []\n",
    "\n",
    "# prepare operator for dimensionality reduction\n",
    "target_intervals = 1\n",
    "lag = 2\n",
    "\n",
    "# cross validation based dimensionality reduction and model training\n",
    "for x_train, x_test, y_train, y_test in cv.split(precursor_field_sel[:-test_samples],\n",
    "                                                 y=target_series_sel[:-test_samples]):\n",
    "    # log train/test splits with anchor years\n",
    "    train_test_splits.append({\n",
    "        \"train\": x_train.anchor_year.values,\n",
    "        \"test\": x_test.anchor_year.values,\n",
    "    })\n",
    "    # fit dimensionality reduction operator RGDR\n",
    "    rgdr = RGDR(\n",
    "        target_intervals=target_intervals,\n",
    "        lag=lag,\n",
    "        eps_km=600,\n",
    "        alpha=0.05,\n",
    "        min_area_km2=0\n",
    "    )\n",
    "    rgdr.fit(x_train, y_train)\n",
    "    # save dimensionality reduction operator\n",
    "    RGDRs.append(rgdr)\n",
    "    # transform to train and test data\n",
    "    clusters_train = rgdr.transform(x_train)\n",
    "    clusters_test = rgdr.transform(x_test)\n",
    "    # train model\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    model = ridge.fit(clusters_train.isel(i_interval=0), y_train.sel(i_interval=1))\n",
    "    # save model\n",
    "    models.append(model)\n",
    "    # predict and save results\n",
    "    prediction = model.predict(clusters_test.isel(i_interval=0))\n",
    "    # calculate and save rmse\n",
    "    rmse_train.append(mean_squared_error(y_train.sel(i_interval=1),\n",
    "                                         model.predict(clusters_train.isel(i_interval=0))))\n",
    "    rmse_test.append(mean_squared_error(y_test.sel(i_interval=1),\n",
    "                                        prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction with the RGDR operator used by the best model\n",
    "clusters_test = RGDRs[np.argmax(rmse_test)].transform(precursor_field_sel[-test_samples:])\n",
    "# predict with the best model\n",
    "predictions_baseline = models[np.argmax(rmse_test)].predict(clusters_test.isel(i_interval=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all predictions and the ground truth and check the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.mean(test_y_torch.squeeze().numpy(), 1)\n",
    "\n",
    "print(\n",
    "    f\"The MSE of LSTM forecasts is {mean_squared_error(ground_truth, np.concatenate(predictions)):.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"The MSE of baseline ridge forecasts is {mean_squared_error(ground_truth, predictions_baseline):.3f}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(instances, np.concatenate(predictions), label=\"Predictions-Transformer\")\n",
    "plt.scatter(instances, predictions_baseline, label=\"Predictions-Ridge\")\n",
    "plt.scatter(instances, ground_truth, label=\"Ground truth\")\n",
    "plt.xlabel(\"Experiment\")\n",
    "plt.ylabel(\"TS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. More machine learning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgdr.fit(precursor_field_sel[:train_samples],\n",
    "         target_series_sel[:train_samples])\n",
    "clusters_precursor = rgdr.transform(precursor_field_sel)\n",
    "data_pd = clusters_precursor.isel(i_interval=0).to_pandas()\n",
    "target_pd = target_series_sel.sel(i_interval=1).to_pandas()\n",
    "data_pd[\"target\"] = target_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycaret regression and init setup\n",
    "from pycaret.regression import *\n",
    "s = setup(data_pd, target = 'target', session_id = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare baseline models\n",
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error\n",
    "plot_model(best, plot = 'error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4s2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
